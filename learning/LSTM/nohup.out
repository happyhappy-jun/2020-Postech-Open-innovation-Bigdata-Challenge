2020-07-30 02:02:13.171514: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-07-30 02:02:13.219166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.911
pciBusID: 0000:05:00.0
2020-07-30 02:02:13.225840: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-07-30 02:02:13.403364: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-07-30 02:02:13.696674: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-07-30 02:02:14.329733: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-07-30 02:02:14.714507: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-07-30 02:02:14.900809: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-07-30 02:02:15.501237: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-30 02:02:15.504366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-07-30 02:02:15.504762: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-07-30 02:02:15.511497: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3597960000 Hz
2020-07-30 02:02:15.512226: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5b5bd40 executing computations on platform Host. Devices:
2020-07-30 02:02:15.512247: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2020-07-30 02:02:15.663413: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5b77660 executing computations on platform CUDA. Devices:
2020-07-30 02:02:15.663463: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): TITAN Xp, Compute Capability 6.1
2020-07-30 02:02:15.666588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.911
pciBusID: 0000:05:00.0
2020-07-30 02:02:15.666637: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-07-30 02:02:15.666655: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-07-30 02:02:15.666669: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-07-30 02:02:15.666684: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-07-30 02:02:15.666697: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-07-30 02:02:15.666711: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-07-30 02:02:15.666726: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-30 02:02:15.669359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-07-30 02:02:15.669399: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-07-30 02:02:15.671294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-30 02:02:15.671309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2020-07-30 02:02:15.671315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2020-07-30 02:02:15.674052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11435 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:05:00.0, compute capability: 6.1)
2020-07-30 02:02:22.988667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.911
pciBusID: 0000:05:00.0
2020-07-30 02:02:22.988721: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-07-30 02:02:22.988736: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-07-30 02:02:22.988748: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-07-30 02:02:22.988759: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-07-30 02:02:22.988771: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-07-30 02:02:22.988783: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-07-30 02:02:22.988795: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-30 02:02:22.990640: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-07-30 02:02:22.990663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-30 02:02:22.990669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2020-07-30 02:02:22.990674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2020-07-30 02:02:22.992597: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:0 with 11435 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:05:00.0, compute capability: 6.1)
2020-07-30 02:02:22.994110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.911
pciBusID: 0000:05:00.0
2020-07-30 02:02:22.994134: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-07-30 02:02:22.994148: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-07-30 02:02:22.994159: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-07-30 02:02:22.994170: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-07-30 02:02:22.994181: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-07-30 02:02:22.994192: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-07-30 02:02:22.994204: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-30 02:02:22.996039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-07-30 02:02:22.996055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-30 02:02:22.996061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2020-07-30 02:02:22.996065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2020-07-30 02:02:22.998137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:0 with 11435 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:05:00.0, compute capability: 6.1)
Using TensorFlow backend.
WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f955c6b1268> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'
2020-07-30 02:02:25.184939: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-07-30 02:02:25.622007: I tensorflow/core/profiler/lib/profiler_session.cc:184] Profiler session started.
2020-07-30 02:02:25.623543: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcupti.so.10.0'; dlerror: libcupti.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ohpc/pub/apps/cuDNN/cuda-10.0/cudnn-7.6.4.38/lib64:/usr/lib64/nvidia:/opt/ohpc/pub/apps/cuda-10.0/targets/x86_64-linux/lib:/opt/ohpc/pub/apps/cuda-10.0/lib64:/opt/ohpc/pub/apps/python3/3.6.9/lib:/opt/ohpc/pub/mpi/openmpi-gnu7/1.10.7/lib:/opt/ohpc/pub/compiler/gcc/7.3.0/lib64
2020-07-30 02:02:25.623570: W tensorflow/core/profiler/lib/profiler_session.cc:192] Encountered error while starting profiler: Unavailable: CUPTI error: CUPTI could not be loaded or symbol could not be found.
2020-07-30 02:02:25.666195: I tensorflow/core/platform/default/device_tracer.cc:588] Collecting 0 kernel records, 0 memcpy records.
2020-07-30 02:02:25.666357: E tensorflow/core/platform/default/device_tracer.cc:70] CUPTI error: CUPTI could not be loaded or symbol could not be found.
[+] Available GPUs
['/device:GPU:0']
[+] Available multiple GPU not found... Just use CPU! XD
Train on 27881 samples, validate on 6971 samples
Epoch 1/1000

Epoch 00001: val_loss improved from inf to 0.00412, saving model to ../23_checkpoint.keras
27881/27881 - 10s - loss: 0.0080 - val_loss: 0.0041
Epoch 2/1000

Epoch 00002: val_loss improved from 0.00412 to 0.00373, saving model to ../23_checkpoint.keras
27881/27881 - 7s - loss: 0.0036 - val_loss: 0.0037
Epoch 3/1000

Epoch 00003: val_loss improved from 0.00373 to 0.00359, saving model to ../23_checkpoint.keras
27881/27881 - 7s - loss: 0.0034 - val_loss: 0.0036
Epoch 4/1000

Epoch 00004: val_loss did not improve from 0.00359
27881/27881 - 7s - loss: 0.0033 - val_loss: 0.0039
Epoch 5/1000

Epoch 00005: val_loss improved from 0.00359 to 0.00348, saving model to ../23_checkpoint.keras
27881/27881 - 7s - loss: 0.0032 - val_loss: 0.0035
Epoch 6/1000

Epoch 00006: val_loss did not improve from 0.00348
27881/27881 - 7s - loss: 0.0031 - val_loss: 0.0037
Epoch 7/1000

Epoch 00007: val_loss did not improve from 0.00348
27881/27881 - 7s - loss: 0.0030 - val_loss: 0.0035
Epoch 8/1000

Epoch 00008: val_loss did not improve from 0.00348
27881/27881 - 7s - loss: 0.0030 - val_loss: 0.0035
Epoch 9/1000

Epoch 00009: val_loss improved from 0.00348 to 0.00347, saving model to ../23_checkpoint.keras
27881/27881 - 7s - loss: 0.0029 - val_loss: 0.0035
Epoch 10/1000

Epoch 00010: val_loss did not improve from 0.00347
27881/27881 - 7s - loss: 0.0028 - val_loss: 0.0035
Epoch 11/1000

Epoch 00011: val_loss improved from 0.00347 to 0.00346, saving model to ../23_checkpoint.keras
27881/27881 - 7s - loss: 0.0028 - val_loss: 0.0035
Epoch 12/1000

Epoch 00012: val_loss improved from 0.00346 to 0.00330, saving model to ../23_checkpoint.keras
27881/27881 - 7s - loss: 0.0027 - val_loss: 0.0033
Epoch 13/1000

Epoch 00013: val_loss did not improve from 0.00330
27881/27881 - 7s - loss: 0.0027 - val_loss: 0.0033
Epoch 14/1000

Epoch 00014: val_loss did not improve from 0.00330
27881/27881 - 7s - loss: 0.0026 - val_loss: 0.0035
Epoch 15/1000

Epoch 00015: val_loss did not improve from 0.00330
27881/27881 - 7s - loss: 0.0026 - val_loss: 0.0036
Epoch 16/1000

Epoch 00016: val_loss improved from 0.00330 to 0.00322, saving model to ../23_checkpoint.keras
27881/27881 - 7s - loss: 0.0026 - val_loss: 0.0032
Epoch 17/1000

Epoch 00017: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0026 - val_loss: 0.0035
Epoch 18/1000

Epoch 00018: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0026 - val_loss: 0.0033
Epoch 19/1000

Epoch 00019: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0025 - val_loss: 0.0042
Epoch 20/1000

Epoch 00020: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0025 - val_loss: 0.0040
Epoch 21/1000

Epoch 00021: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0025 - val_loss: 0.0037
Epoch 22/1000

Epoch 00022: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0025 - val_loss: 0.0042
Epoch 23/1000

Epoch 00023: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0025 - val_loss: 0.0037
Epoch 24/1000

Epoch 00024: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0025 - val_loss: 0.0034
Epoch 25/1000

Epoch 00025: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0025 - val_loss: 0.0035
Epoch 26/1000

Epoch 00026: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0025 - val_loss: 0.0039
Epoch 27/1000

Epoch 00027: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0025 - val_loss: 0.0039
Epoch 28/1000

Epoch 00028: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0025 - val_loss: 0.0035
Epoch 29/1000

Epoch 00029: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0024 - val_loss: 0.0035
Epoch 30/1000

Epoch 00030: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0024 - val_loss: 0.0036
Epoch 31/1000

Epoch 00031: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0024 - val_loss: 0.0038
Epoch 32/1000

Epoch 00032: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0024 - val_loss: 0.0035
Epoch 33/1000

Epoch 00033: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0024 - val_loss: 0.0036
Epoch 34/1000

Epoch 00034: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0024 - val_loss: 0.0038
Epoch 35/1000

Epoch 00035: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0024 - val_loss: 0.0034
Epoch 36/1000

Epoch 00036: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0024 - val_loss: 0.0036
Epoch 37/1000

Epoch 00037: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0024 - val_loss: 0.0034
Epoch 38/1000

Epoch 00038: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0024 - val_loss: 0.0035
Epoch 39/1000

Epoch 00039: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0024 - val_loss: 0.0035
Epoch 40/1000

Epoch 00040: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0024 - val_loss: 0.0039
Epoch 41/1000

Epoch 00041: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0024 - val_loss: 0.0037
Epoch 42/1000

Epoch 00042: val_loss did not improve from 0.00322

Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
27881/27881 - 7s - loss: 0.0024 - val_loss: 0.0035
Epoch 43/1000

Epoch 00043: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0036
Epoch 44/1000

Epoch 00044: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0023 - val_loss: 0.0037
Epoch 45/1000

Epoch 00045: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0034
Epoch 46/1000

Epoch 00046: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0037
Epoch 47/1000

Epoch 00047: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0036
Epoch 48/1000

Epoch 00048: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0036
Epoch 49/1000

Epoch 00049: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0037
Epoch 50/1000

Epoch 00050: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0034
Epoch 51/1000

Epoch 00051: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0037
Epoch 52/1000

Epoch 00052: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0036
Epoch 53/1000

Epoch 00053: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0037
Epoch 54/1000

Epoch 00054: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0044
Epoch 55/1000

Epoch 00055: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0041
Epoch 56/1000

Epoch 00056: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0041
Epoch 57/1000

Epoch 00057: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0036
Epoch 58/1000

Epoch 00058: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0040
Epoch 59/1000

Epoch 00059: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0036
Epoch 60/1000

Epoch 00060: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0038
Epoch 61/1000

Epoch 00061: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0039
Epoch 62/1000

Epoch 00062: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0042
Epoch 63/1000

Epoch 00063: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0038
Epoch 64/1000

Epoch 00064: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0049
Epoch 65/1000

Epoch 00065: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0039
Epoch 66/1000

Epoch 00066: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0039
Epoch 67/1000

Epoch 00067: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0040
Epoch 68/1000

Epoch 00068: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0037
Epoch 69/1000

Epoch 00069: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0042
Epoch 70/1000

Epoch 00070: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0035
Epoch 71/1000

Epoch 00071: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0037
Epoch 72/1000

Epoch 00072: val_loss did not improve from 0.00322

Epoch 00072: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0038
Epoch 73/1000

Epoch 00073: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0037
Epoch 74/1000

Epoch 00074: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0039
Epoch 75/1000

Epoch 00075: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0044
Epoch 76/1000

Epoch 00076: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0038
Epoch 77/1000

Epoch 00077: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0037
Epoch 78/1000

Epoch 00078: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0037
Epoch 79/1000

Epoch 00079: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0036
Epoch 80/1000

Epoch 00080: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0043
Epoch 81/1000

Epoch 00081: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0040
Epoch 82/1000

Epoch 00082: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0041
Epoch 83/1000

Epoch 00083: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0039
Epoch 84/1000

Epoch 00084: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0037
Epoch 85/1000

Epoch 00085: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0036
Epoch 86/1000

Epoch 00086: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0038
Epoch 87/1000

Epoch 00087: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0038
Epoch 88/1000

Epoch 00088: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0039
Epoch 89/1000

Epoch 00089: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0038
Epoch 90/1000

Epoch 00090: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0037
Epoch 91/1000

Epoch 00091: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0037
Epoch 92/1000

Epoch 00092: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0037
Epoch 93/1000

Epoch 00093: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0039
Epoch 94/1000

Epoch 00094: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0042
Epoch 95/1000

Epoch 00095: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0038
Epoch 96/1000

Epoch 00096: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0038
Epoch 97/1000

Epoch 00097: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0037
Epoch 98/1000

Epoch 00098: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0041
Epoch 99/1000

Epoch 00099: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0036
Epoch 100/1000

Epoch 00100: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0036
Epoch 101/1000

Epoch 00101: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0037
Epoch 102/1000

Epoch 00102: val_loss did not improve from 0.00322

Epoch 00102: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0037
Epoch 103/1000

Epoch 00103: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 104/1000

Epoch 00104: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 105/1000

Epoch 00105: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 106/1000

Epoch 00106: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0038
Epoch 107/1000

Epoch 00107: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0040
Epoch 108/1000

Epoch 00108: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0038
Epoch 109/1000

Epoch 00109: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0038
Epoch 110/1000

Epoch 00110: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0040
Epoch 111/1000

Epoch 00111: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0040
Epoch 112/1000

Epoch 00112: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0038
Epoch 113/1000

Epoch 00113: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 114/1000

Epoch 00114: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 115/1000

Epoch 00115: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 116/1000

Epoch 00116: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0038
Epoch 117/1000

Epoch 00117: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0041
Epoch 118/1000

Epoch 00118: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0038
Epoch 119/1000

Epoch 00119: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0040
Epoch 120/1000

Epoch 00120: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 121/1000

Epoch 00121: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0037
Epoch 122/1000

Epoch 00122: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 123/1000

Epoch 00123: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0040
Epoch 124/1000

Epoch 00124: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 125/1000

Epoch 00125: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0040
Epoch 126/1000

Epoch 00126: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 127/1000

Epoch 00127: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 128/1000

Epoch 00128: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0038
Epoch 129/1000

Epoch 00129: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0040
Epoch 130/1000

Epoch 00130: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0038
Epoch 131/1000

Epoch 00131: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0040
Epoch 132/1000

Epoch 00132: val_loss did not improve from 0.00322

Epoch 00132: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0040
Epoch 133/1000

Epoch 00133: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 134/1000

Epoch 00134: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 135/1000

Epoch 00135: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0038
Epoch 136/1000

Epoch 00136: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0038
Epoch 137/1000

Epoch 00137: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 138/1000

Epoch 00138: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0038
Epoch 139/1000

Epoch 00139: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 140/1000

Epoch 00140: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0038
Epoch 141/1000

Epoch 00141: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 142/1000

Epoch 00142: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0038
Epoch 143/1000

Epoch 00143: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 144/1000

Epoch 00144: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0040
Epoch 145/1000

Epoch 00145: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 146/1000

Epoch 00146: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 147/1000

Epoch 00147: val_loss did not improve from 0.00322
27881/27881 - 6s - loss: 0.0020 - val_loss: 0.0039
Epoch 148/1000

Epoch 00148: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 149/1000

Epoch 00149: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0041
Epoch 150/1000

Epoch 00150: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 151/1000

Epoch 00151: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 152/1000

Epoch 00152: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 153/1000

Epoch 00153: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0038
Epoch 154/1000

Epoch 00154: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0040
Epoch 155/1000

Epoch 00155: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 156/1000

Epoch 00156: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 157/1000

Epoch 00157: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 158/1000

Epoch 00158: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0041
Epoch 159/1000

Epoch 00159: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0038
Epoch 160/1000

Epoch 00160: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 161/1000

Epoch 00161: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0038
Epoch 162/1000

Epoch 00162: val_loss did not improve from 0.00322

Epoch 00162: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0040
Epoch 163/1000

Epoch 00163: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0040
Epoch 164/1000

Epoch 00164: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 165/1000

Epoch 00165: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 166/1000

Epoch 00166: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0038
Epoch 167/1000

Epoch 00167: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 168/1000

Epoch 00168: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 169/1000

Epoch 00169: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0038
Epoch 170/1000

Epoch 00170: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 171/1000

Epoch 00171: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 172/1000

Epoch 00172: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 173/1000

Epoch 00173: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 174/1000

Epoch 00174: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 175/1000

Epoch 00175: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 176/1000

Epoch 00176: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 177/1000

Epoch 00177: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0040
Epoch 178/1000

Epoch 00178: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 179/1000

Epoch 00179: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0040
Epoch 180/1000

Epoch 00180: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 181/1000

Epoch 00181: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 182/1000

Epoch 00182: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 183/1000

Epoch 00183: val_loss did not improve from 0.00322
27881/27881 - 6s - loss: 0.0020 - val_loss: 0.0039
Epoch 184/1000

Epoch 00184: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0038
Epoch 185/1000

Epoch 00185: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0040
Epoch 186/1000

Epoch 00186: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0038
Epoch 187/1000

Epoch 00187: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 188/1000

Epoch 00188: val_loss did not improve from 0.00322
27881/27881 - 6s - loss: 0.0020 - val_loss: 0.0039
Epoch 189/1000

Epoch 00189: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0040
Epoch 190/1000

Epoch 00190: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 191/1000

Epoch 00191: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0040
Epoch 192/1000

Epoch 00192: val_loss did not improve from 0.00322

Epoch 00192: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 193/1000

Epoch 00193: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0040
Epoch 194/1000

Epoch 00194: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 195/1000

Epoch 00195: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0040
Epoch 196/1000

Epoch 00196: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 197/1000

Epoch 00197: val_loss did not improve from 0.00322
27881/27881 - 6s - loss: 0.0020 - val_loss: 0.0039
Epoch 198/1000

Epoch 00198: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0040
Epoch 199/1000

Epoch 00199: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 200/1000

Epoch 00200: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0040
Epoch 201/1000

Epoch 00201: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 202/1000

Epoch 00202: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 203/1000

Epoch 00203: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 204/1000

Epoch 00204: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0040
Epoch 205/1000

Epoch 00205: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 206/1000

Epoch 00206: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 207/1000

Epoch 00207: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0040
Epoch 208/1000

Epoch 00208: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0040
Epoch 209/1000

Epoch 00209: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 210/1000

Epoch 00210: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 211/1000

Epoch 00211: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 212/1000

Epoch 00212: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 213/1000

Epoch 00213: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 214/1000

Epoch 00214: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0040
Epoch 215/1000

Epoch 00215: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 216/1000

Epoch 00216: val_loss did not improve from 0.00322
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0039
Epoch 00216: early stopping
after concate: (96, 10)
Test RMSE: 5.068814
Test MAE: 25.692877
after concate: (96, 10)
Test RMSE: 11.874458
Test MAE: 141.002753
after concate: (96, 10)
Test RMSE: 10.016120
Test MAE: 100.322657
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru (GRU)                    (None, 1, 32)             4128      
_________________________________________________________________
gru_1 (GRU)                  (None, 1, 32)             6336      
_________________________________________________________________
dense (Dense)                (None, 1, 1)              33        
=================================================================
Total params: 10,497
Trainable params: 10,497
Non-trainable params: 0
_________________________________________________________________
None
after concate: (96, 10)
Test RMSE: 9.419522
Test MAE: 88.727387
2020-07-30 02:32:10.151665: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-07-30 02:32:10.206235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.911
pciBusID: 0000:05:00.0
2020-07-30 02:32:10.207144: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-07-30 02:32:10.208952: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-07-30 02:32:10.210627: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-07-30 02:32:10.211402: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-07-30 02:32:10.213412: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-07-30 02:32:10.214994: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-07-30 02:32:10.219361: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-30 02:32:10.222660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-07-30 02:32:10.223025: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-07-30 02:32:10.229896: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3597960000 Hz
2020-07-30 02:32:10.230487: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x733ea80 executing computations on platform Host. Devices:
2020-07-30 02:32:10.230505: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2020-07-30 02:32:10.338134: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x735a340 executing computations on platform CUDA. Devices:
2020-07-30 02:32:10.338184: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): TITAN Xp, Compute Capability 6.1
2020-07-30 02:32:10.341106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.911
pciBusID: 0000:05:00.0
2020-07-30 02:32:10.341173: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-07-30 02:32:10.341204: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-07-30 02:32:10.341232: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-07-30 02:32:10.341259: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-07-30 02:32:10.341285: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-07-30 02:32:10.341312: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-07-30 02:32:10.341340: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-30 02:32:10.346467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-07-30 02:32:10.346505: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-07-30 02:32:10.348422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-30 02:32:10.348436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2020-07-30 02:32:10.348442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2020-07-30 02:32:10.351166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11435 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:05:00.0, compute capability: 6.1)
2020-07-30 02:32:11.159238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.911
pciBusID: 0000:05:00.0
2020-07-30 02:32:11.159293: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-07-30 02:32:11.159308: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-07-30 02:32:11.159321: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-07-30 02:32:11.159334: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-07-30 02:32:11.159346: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-07-30 02:32:11.159358: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-07-30 02:32:11.159371: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-30 02:32:11.161318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-07-30 02:32:11.161342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-30 02:32:11.161348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2020-07-30 02:32:11.161353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2020-07-30 02:32:11.163360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:0 with 11435 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:05:00.0, compute capability: 6.1)
2020-07-30 02:32:11.164917: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.911
pciBusID: 0000:05:00.0
2020-07-30 02:32:11.164942: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-07-30 02:32:11.164956: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-07-30 02:32:11.164968: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-07-30 02:32:11.164980: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-07-30 02:32:11.164993: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-07-30 02:32:11.165005: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-07-30 02:32:11.165017: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-30 02:32:11.166959: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-07-30 02:32:11.166977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-30 02:32:11.166987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2020-07-30 02:32:11.166992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2020-07-30 02:32:11.171922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:0 with 11435 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:05:00.0, compute capability: 6.1)
Using TensorFlow backend.
WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fab803bf268> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'
2020-07-30 02:32:13.331499: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-07-30 02:32:13.521207: I tensorflow/core/profiler/lib/profiler_session.cc:184] Profiler session started.
2020-07-30 02:32:13.521448: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcupti.so.10.0'; dlerror: libcupti.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ohpc/pub/apps/cuDNN/cuda-10.0/cudnn-7.6.4.38/lib64:/usr/lib64/nvidia:/opt/ohpc/pub/apps/cuda-10.0/targets/x86_64-linux/lib:/opt/ohpc/pub/apps/cuda-10.0/lib64:/opt/ohpc/pub/apps/python3/3.6.9/lib:/opt/ohpc/pub/mpi/openmpi-gnu7/1.10.7/lib:/opt/ohpc/pub/compiler/gcc/7.3.0/lib64
2020-07-30 02:32:13.521473: W tensorflow/core/profiler/lib/profiler_session.cc:192] Encountered error while starting profiler: Unavailable: CUPTI error: CUPTI could not be loaded or symbol could not be found.
2020-07-30 02:32:13.566779: I tensorflow/core/platform/default/device_tracer.cc:588] Collecting 0 kernel records, 0 memcpy records.
2020-07-30 02:32:13.566936: E tensorflow/core/platform/default/device_tracer.cc:70] CUPTI error: CUPTI could not be loaded or symbol could not be found.
[+] Available GPUs
['/device:GPU:0']
[+] Available multiple GPU not found... Just use CPU! XD
Train on 27881 samples, validate on 6971 samples
Epoch 1/1000

Epoch 00001: val_loss improved from inf to 0.00380, saving model to ../23_checkpoint.keras
27881/27881 - 9s - loss: 0.0050 - val_loss: 0.0038
Epoch 2/1000

Epoch 00002: val_loss improved from 0.00380 to 0.00352, saving model to ../23_checkpoint.keras
27881/27881 - 7s - loss: 0.0033 - val_loss: 0.0035
Epoch 3/1000

Epoch 00003: val_loss did not improve from 0.00352
27881/27881 - 7s - loss: 0.0032 - val_loss: 0.0039
Epoch 4/1000

Epoch 00004: val_loss improved from 0.00352 to 0.00345, saving model to ../23_checkpoint.keras
27881/27881 - 7s - loss: 0.0030 - val_loss: 0.0034
Epoch 5/1000

Epoch 00005: val_loss did not improve from 0.00345
27881/27881 - 7s - loss: 0.0029 - val_loss: 0.0037
Epoch 6/1000

Epoch 00006: val_loss did not improve from 0.00345
27881/27881 - 7s - loss: 0.0028 - val_loss: 0.0035
Epoch 7/1000

Epoch 00007: val_loss improved from 0.00345 to 0.00344, saving model to ../23_checkpoint.keras
27881/27881 - 7s - loss: 0.0028 - val_loss: 0.0034
Epoch 8/1000

Epoch 00008: val_loss improved from 0.00344 to 0.00342, saving model to ../23_checkpoint.keras
27881/27881 - 7s - loss: 0.0027 - val_loss: 0.0034
Epoch 9/1000

Epoch 00009: val_loss did not improve from 0.00342
27881/27881 - 7s - loss: 0.0027 - val_loss: 0.0035
Epoch 10/1000

Epoch 00010: val_loss did not improve from 0.00342
27881/27881 - 7s - loss: 0.0026 - val_loss: 0.0035
Epoch 11/1000

Epoch 00011: val_loss improved from 0.00342 to 0.00339, saving model to ../23_checkpoint.keras
27881/27881 - 7s - loss: 0.0026 - val_loss: 0.0034
Epoch 12/1000

Epoch 00012: val_loss did not improve from 0.00339
27881/27881 - 7s - loss: 0.0025 - val_loss: 0.0038
Epoch 13/1000

Epoch 00013: val_loss improved from 0.00339 to 0.00324, saving model to ../23_checkpoint.keras
27881/27881 - 7s - loss: 0.0025 - val_loss: 0.0032
Epoch 14/1000

Epoch 00014: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0025 - val_loss: 0.0032
Epoch 15/1000

Epoch 00015: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0025 - val_loss: 0.0034
Epoch 16/1000

Epoch 00016: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0024 - val_loss: 0.0034
Epoch 17/1000

Epoch 00017: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0024 - val_loss: 0.0036
Epoch 18/1000

Epoch 00018: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0024 - val_loss: 0.0033
Epoch 19/1000

Epoch 00019: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0024 - val_loss: 0.0043
Epoch 20/1000

Epoch 00020: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0024 - val_loss: 0.0048
Epoch 21/1000

Epoch 00021: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0023 - val_loss: 0.0035
Epoch 22/1000

Epoch 00022: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0023 - val_loss: 0.0043
Epoch 23/1000

Epoch 00023: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0023 - val_loss: 0.0038
Epoch 24/1000

Epoch 00024: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0023 - val_loss: 0.0036
Epoch 25/1000

Epoch 00025: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0038
Epoch 26/1000

Epoch 00026: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0041
Epoch 27/1000

Epoch 00027: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0036
Epoch 28/1000

Epoch 00028: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0039
Epoch 29/1000

Epoch 00029: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0022 - val_loss: 0.0035
Epoch 30/1000

Epoch 00030: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0038
Epoch 31/1000

Epoch 00031: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0039
Epoch 32/1000

Epoch 00032: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0034
Epoch 33/1000

Epoch 00033: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0035
Epoch 34/1000

Epoch 00034: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0048
Epoch 35/1000

Epoch 00035: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0021 - val_loss: 0.0035
Epoch 36/1000

Epoch 00036: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0035
Epoch 37/1000

Epoch 00037: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0036
Epoch 38/1000

Epoch 00038: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0035
Epoch 39/1000

Epoch 00039: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0035
Epoch 40/1000

Epoch 00040: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0045
Epoch 41/1000

Epoch 00041: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0020 - val_loss: 0.0032
Epoch 42/1000

Epoch 00042: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0019 - val_loss: 0.0038
Epoch 43/1000

Epoch 00043: val_loss did not improve from 0.00324

Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
27881/27881 - 7s - loss: 0.0019 - val_loss: 0.0036
Epoch 44/1000

Epoch 00044: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0018 - val_loss: 0.0037
Epoch 45/1000

Epoch 00045: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0017 - val_loss: 0.0037
Epoch 46/1000

Epoch 00046: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0017 - val_loss: 0.0040
Epoch 47/1000

Epoch 00047: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0017 - val_loss: 0.0040
Epoch 48/1000

Epoch 00048: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0017 - val_loss: 0.0037
Epoch 49/1000

Epoch 00049: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0017 - val_loss: 0.0039
Epoch 50/1000

Epoch 00050: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0017 - val_loss: 0.0035
Epoch 51/1000

Epoch 00051: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0017 - val_loss: 0.0038
Epoch 52/1000

Epoch 00052: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0016 - val_loss: 0.0036
Epoch 53/1000

Epoch 00053: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0016 - val_loss: 0.0038
Epoch 54/1000

Epoch 00054: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0016 - val_loss: 0.0044
Epoch 55/1000

Epoch 00055: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0016 - val_loss: 0.0042
Epoch 56/1000

Epoch 00056: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0016 - val_loss: 0.0037
Epoch 57/1000

Epoch 00057: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0016 - val_loss: 0.0041
Epoch 58/1000

Epoch 00058: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0016 - val_loss: 0.0042
Epoch 59/1000

Epoch 00059: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0016 - val_loss: 0.0037
Epoch 60/1000

Epoch 00060: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0016 - val_loss: 0.0044
Epoch 61/1000

Epoch 00061: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0015 - val_loss: 0.0037
Epoch 62/1000

Epoch 00062: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0015 - val_loss: 0.0040
Epoch 63/1000

Epoch 00063: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0015 - val_loss: 0.0039
Epoch 64/1000

Epoch 00064: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0015 - val_loss: 0.0053
Epoch 65/1000

Epoch 00065: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0015 - val_loss: 0.0042
Epoch 66/1000

Epoch 00066: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0015 - val_loss: 0.0044
Epoch 67/1000

Epoch 00067: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0015 - val_loss: 0.0043
Epoch 68/1000

Epoch 00068: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0015 - val_loss: 0.0042
Epoch 69/1000

Epoch 00069: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0015 - val_loss: 0.0049
Epoch 70/1000

Epoch 00070: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0015 - val_loss: 0.0042
Epoch 71/1000

Epoch 00071: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0015 - val_loss: 0.0042
Epoch 72/1000

Epoch 00072: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0015 - val_loss: 0.0040
Epoch 73/1000

Epoch 00073: val_loss did not improve from 0.00324

Epoch 00073: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
27881/27881 - 6s - loss: 0.0014 - val_loss: 0.0040
Epoch 74/1000

Epoch 00074: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0013 - val_loss: 0.0044
Epoch 75/1000

Epoch 00075: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0013 - val_loss: 0.0043
Epoch 76/1000

Epoch 00076: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0013 - val_loss: 0.0040
Epoch 77/1000

Epoch 00077: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0013 - val_loss: 0.0043
Epoch 78/1000

Epoch 00078: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0013 - val_loss: 0.0046
Epoch 79/1000

Epoch 00079: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0013 - val_loss: 0.0041
Epoch 80/1000

Epoch 00080: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0013 - val_loss: 0.0049
Epoch 81/1000

Epoch 00081: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0013 - val_loss: 0.0045
Epoch 82/1000

Epoch 00082: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0013 - val_loss: 0.0045
Epoch 83/1000

Epoch 00083: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0013 - val_loss: 0.0043
Epoch 84/1000

Epoch 00084: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0013 - val_loss: 0.0040
Epoch 85/1000

Epoch 00085: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0013 - val_loss: 0.0045
Epoch 86/1000

Epoch 00086: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0013 - val_loss: 0.0046
Epoch 87/1000

Epoch 00087: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0013 - val_loss: 0.0041
Epoch 88/1000

Epoch 00088: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0012 - val_loss: 0.0046
Epoch 89/1000

Epoch 00089: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0012 - val_loss: 0.0045
Epoch 90/1000

Epoch 00090: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0012 - val_loss: 0.0043
Epoch 91/1000

Epoch 00091: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0012 - val_loss: 0.0041
Epoch 92/1000

Epoch 00092: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0012 - val_loss: 0.0043
Epoch 93/1000

Epoch 00093: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0012 - val_loss: 0.0046
Epoch 94/1000

Epoch 00094: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0012 - val_loss: 0.0047
Epoch 95/1000

Epoch 00095: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0012 - val_loss: 0.0045
Epoch 96/1000

Epoch 00096: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0012 - val_loss: 0.0041
Epoch 97/1000

Epoch 00097: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0012 - val_loss: 0.0042
Epoch 98/1000

Epoch 00098: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0012 - val_loss: 0.0045
Epoch 99/1000

Epoch 00099: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0012 - val_loss: 0.0043
Epoch 100/1000

Epoch 00100: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0012 - val_loss: 0.0042
Epoch 101/1000

Epoch 00101: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0012 - val_loss: 0.0044
Epoch 102/1000

Epoch 00102: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0012 - val_loss: 0.0043
Epoch 103/1000

Epoch 00103: val_loss did not improve from 0.00324

Epoch 00103: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
27881/27881 - 7s - loss: 0.0012 - val_loss: 0.0045
Epoch 104/1000

Epoch 00104: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0011 - val_loss: 0.0045
Epoch 105/1000

Epoch 00105: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0011 - val_loss: 0.0046
Epoch 106/1000

Epoch 00106: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0011 - val_loss: 0.0044
Epoch 107/1000

Epoch 00107: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0011 - val_loss: 0.0047
Epoch 108/1000

Epoch 00108: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0011 - val_loss: 0.0044
Epoch 109/1000

Epoch 00109: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0011 - val_loss: 0.0044
Epoch 110/1000

Epoch 00110: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0011 - val_loss: 0.0045
Epoch 111/1000

Epoch 00111: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0011 - val_loss: 0.0045
Epoch 112/1000

Epoch 00112: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0011 - val_loss: 0.0047
Epoch 113/1000

Epoch 00113: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0011 - val_loss: 0.0046
Epoch 114/1000

Epoch 00114: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0011 - val_loss: 0.0045
Epoch 115/1000

Epoch 00115: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0011 - val_loss: 0.0050
Epoch 116/1000

Epoch 00116: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0011 - val_loss: 0.0049
Epoch 117/1000

Epoch 00117: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0011 - val_loss: 0.0046
Epoch 118/1000

Epoch 00118: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0011 - val_loss: 0.0046
Epoch 119/1000

Epoch 00119: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0011 - val_loss: 0.0045
Epoch 120/1000

Epoch 00120: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 0.0011 - val_loss: 0.0050
Epoch 121/1000

Epoch 00121: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0011 - val_loss: 0.0046
Epoch 122/1000

Epoch 00122: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0011 - val_loss: 0.0047
Epoch 123/1000

Epoch 00123: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0011 - val_loss: 0.0046
Epoch 124/1000

Epoch 00124: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0010 - val_loss: 0.0047
Epoch 125/1000

Epoch 00125: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0010 - val_loss: 0.0047
Epoch 126/1000

Epoch 00126: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0010 - val_loss: 0.0046
Epoch 127/1000

Epoch 00127: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0010 - val_loss: 0.0049
Epoch 128/1000

Epoch 00128: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0010 - val_loss: 0.0048
Epoch 129/1000

Epoch 00129: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0010 - val_loss: 0.0047
Epoch 130/1000

Epoch 00130: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0010 - val_loss: 0.0045
Epoch 131/1000

Epoch 00131: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0010 - val_loss: 0.0051
Epoch 132/1000

Epoch 00132: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 0.0010 - val_loss: 0.0048
Epoch 133/1000

Epoch 00133: val_loss did not improve from 0.00324

Epoch 00133: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.
27881/27881 - 6s - loss: 0.0010 - val_loss: 0.0050
Epoch 134/1000

Epoch 00134: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.9029e-04 - val_loss: 0.0047
Epoch 135/1000

Epoch 00135: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 9.8479e-04 - val_loss: 0.0047
Epoch 136/1000

Epoch 00136: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.8573e-04 - val_loss: 0.0047
Epoch 137/1000

Epoch 00137: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 9.8385e-04 - val_loss: 0.0047
Epoch 138/1000

Epoch 00138: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.8211e-04 - val_loss: 0.0048
Epoch 139/1000

Epoch 00139: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.7685e-04 - val_loss: 0.0048
Epoch 140/1000

Epoch 00140: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.7819e-04 - val_loss: 0.0048
Epoch 141/1000

Epoch 00141: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.7737e-04 - val_loss: 0.0047
Epoch 142/1000

Epoch 00142: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.7565e-04 - val_loss: 0.0048
Epoch 143/1000

Epoch 00143: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.7318e-04 - val_loss: 0.0049
Epoch 144/1000

Epoch 00144: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.7314e-04 - val_loss: 0.0049
Epoch 145/1000

Epoch 00145: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.6962e-04 - val_loss: 0.0048
Epoch 146/1000

Epoch 00146: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.6842e-04 - val_loss: 0.0048
Epoch 147/1000

Epoch 00147: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.6747e-04 - val_loss: 0.0049
Epoch 148/1000

Epoch 00148: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.6453e-04 - val_loss: 0.0048
Epoch 149/1000

Epoch 00149: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.6404e-04 - val_loss: 0.0050
Epoch 150/1000

Epoch 00150: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.5928e-04 - val_loss: 0.0046
Epoch 151/1000

Epoch 00151: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.6270e-04 - val_loss: 0.0050
Epoch 152/1000

Epoch 00152: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.6125e-04 - val_loss: 0.0048
Epoch 153/1000

Epoch 00153: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.6056e-04 - val_loss: 0.0048
Epoch 154/1000

Epoch 00154: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.5673e-04 - val_loss: 0.0048
Epoch 155/1000

Epoch 00155: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.5494e-04 - val_loss: 0.0049
Epoch 156/1000

Epoch 00156: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.5584e-04 - val_loss: 0.0049
Epoch 157/1000

Epoch 00157: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.5357e-04 - val_loss: 0.0050
Epoch 158/1000

Epoch 00158: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.5234e-04 - val_loss: 0.0050
Epoch 159/1000

Epoch 00159: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.5132e-04 - val_loss: 0.0049
Epoch 160/1000

Epoch 00160: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.4768e-04 - val_loss: 0.0049
Epoch 161/1000

Epoch 00161: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.4900e-04 - val_loss: 0.0048
Epoch 162/1000

Epoch 00162: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.4529e-04 - val_loss: 0.0050
Epoch 163/1000

Epoch 00163: val_loss did not improve from 0.00324

Epoch 00163: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.
27881/27881 - 6s - loss: 9.4430e-04 - val_loss: 0.0049
Epoch 164/1000

Epoch 00164: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.2790e-04 - val_loss: 0.0049
Epoch 165/1000

Epoch 00165: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.2537e-04 - val_loss: 0.0048
Epoch 166/1000

Epoch 00166: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.2421e-04 - val_loss: 0.0049
Epoch 167/1000

Epoch 00167: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.2554e-04 - val_loss: 0.0049
Epoch 168/1000

Epoch 00168: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.2293e-04 - val_loss: 0.0048
Epoch 169/1000

Epoch 00169: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.2075e-04 - val_loss: 0.0048
Epoch 170/1000

Epoch 00170: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.2243e-04 - val_loss: 0.0050
Epoch 171/1000

Epoch 00171: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.2187e-04 - val_loss: 0.0049
Epoch 172/1000

Epoch 00172: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.2025e-04 - val_loss: 0.0049
Epoch 173/1000

Epoch 00173: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 9.2002e-04 - val_loss: 0.0049
Epoch 174/1000

Epoch 00174: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.1790e-04 - val_loss: 0.0049
Epoch 175/1000

Epoch 00175: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.1695e-04 - val_loss: 0.0049
Epoch 176/1000

Epoch 00176: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.1809e-04 - val_loss: 0.0050
Epoch 177/1000

Epoch 00177: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.1679e-04 - val_loss: 0.0050
Epoch 178/1000

Epoch 00178: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.1588e-04 - val_loss: 0.0049
Epoch 179/1000

Epoch 00179: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.1416e-04 - val_loss: 0.0049
Epoch 180/1000

Epoch 00180: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.1408e-04 - val_loss: 0.0049
Epoch 181/1000

Epoch 00181: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.1423e-04 - val_loss: 0.0049
Epoch 182/1000

Epoch 00182: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.1199e-04 - val_loss: 0.0050
Epoch 183/1000

Epoch 00183: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.1144e-04 - val_loss: 0.0050
Epoch 184/1000

Epoch 00184: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.1224e-04 - val_loss: 0.0049
Epoch 185/1000

Epoch 00185: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.0946e-04 - val_loss: 0.0051
Epoch 186/1000

Epoch 00186: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.0960e-04 - val_loss: 0.0048
Epoch 187/1000

Epoch 00187: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.0728e-04 - val_loss: 0.0051
Epoch 188/1000

Epoch 00188: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.0834e-04 - val_loss: 0.0051
Epoch 189/1000

Epoch 00189: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.0505e-04 - val_loss: 0.0049
Epoch 190/1000

Epoch 00190: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.0688e-04 - val_loss: 0.0049
Epoch 191/1000

Epoch 00191: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.0609e-04 - val_loss: 0.0051
Epoch 192/1000

Epoch 00192: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 9.0515e-04 - val_loss: 0.0050
Epoch 193/1000

Epoch 00193: val_loss did not improve from 0.00324

Epoch 00193: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.
27881/27881 - 6s - loss: 9.0517e-04 - val_loss: 0.0049
Epoch 194/1000

Epoch 00194: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 8.9585e-04 - val_loss: 0.0049
Epoch 195/1000

Epoch 00195: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 8.9446e-04 - val_loss: 0.0050
Epoch 196/1000

Epoch 00196: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 8.9364e-04 - val_loss: 0.0049
Epoch 197/1000

Epoch 00197: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 8.9469e-04 - val_loss: 0.0050
Epoch 198/1000

Epoch 00198: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 8.9341e-04 - val_loss: 0.0051
Epoch 199/1000

Epoch 00199: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 8.9368e-04 - val_loss: 0.0050
Epoch 200/1000

Epoch 00200: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 8.9241e-04 - val_loss: 0.0050
Epoch 201/1000

Epoch 00201: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 8.9282e-04 - val_loss: 0.0050
Epoch 202/1000

Epoch 00202: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 8.9153e-04 - val_loss: 0.0050
Epoch 203/1000

Epoch 00203: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 8.9131e-04 - val_loss: 0.0050
Epoch 204/1000

Epoch 00204: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 8.9125e-04 - val_loss: 0.0050
Epoch 205/1000

Epoch 00205: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 8.9034e-04 - val_loss: 0.0050
Epoch 206/1000

Epoch 00206: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 8.8997e-04 - val_loss: 0.0050
Epoch 207/1000

Epoch 00207: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 8.8989e-04 - val_loss: 0.0050
Epoch 208/1000

Epoch 00208: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 8.8987e-04 - val_loss: 0.0050
Epoch 209/1000

Epoch 00209: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 8.8832e-04 - val_loss: 0.0050
Epoch 210/1000

Epoch 00210: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 8.8831e-04 - val_loss: 0.0050
Epoch 211/1000

Epoch 00211: val_loss did not improve from 0.00324
27881/27881 - 7s - loss: 8.8874e-04 - val_loss: 0.0050
Epoch 212/1000

Epoch 00212: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 8.8784e-04 - val_loss: 0.0050
Epoch 213/1000

Epoch 00213: val_loss did not improve from 0.00324
27881/27881 - 6s - loss: 8.8722e-04 - val_loss: 0.0050
Epoch 00213: early stopping
after concate: (96, 10)
Test RMSE: 5.518450
Test MAE: 30.453289
after concate: (96, 10)
Test RMSE: 11.891771
Test MAE: 141.414219
after concate: (96, 10)
Test RMSE: 9.855657
Test MAE: 97.133982
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru (GRU)                    (None, 1, 350)            379050    
_________________________________________________________________
gru_1 (GRU)                  (None, 1, 350)            737100    
_________________________________________________________________
dense (Dense)                (None, 1, 1)              351       
=================================================================
Total params: 1,116,501
Trainable params: 1,116,501
Non-trainable params: 0
_________________________________________________________________
None
after concate: (96, 10)
Test RMSE: 9.190226
Test MAE: 84.460253
