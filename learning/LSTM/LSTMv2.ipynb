{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "tf.random.set_seed(10)\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 10000\n",
    "EPOCH = 1000\n",
    "past_history = 4 * 24 * 10\n",
    "future_target = 4 * 24\n",
    "STEP = 4\n",
    "SHIFT_STEP = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "def root_mean_squared_error_loss(y_true, y_pred):\n",
    "    return tf.keras.backend.sqrt(tf.keras.losses.MSE(y_true, y_pred))\n",
    "\n",
    "\n",
    "tf.random.set_seed(10)\n",
    "raw_df = pd.read_csv(\"../../data/datefrom1st.csv\")\n",
    "raw_df.index = raw_df.datetime\n",
    "\n",
    "df = raw_df.iloc[:20640]\n",
    "\n",
    "df = df.drop(\n",
    "    [\"Unnamed: 0\", 'datetime', 'percipitation', 'air_pressure', 'sea_level_pressure',\n",
    "     'wind_degree'], axis=1)\n",
    "df[\"difference\"] = df.astype('int32')\n",
    "df['shift1'] = df['result'].shift(-SHIFT_STEP)\n",
    "df['shift2'] = df['result'].shift(-(SHIFT_STEP+1))\n",
    "df['shift3'] = df['result'].shift(-(SHIFT_STEP+2))\n",
    "df = df.fillna(0)\n",
    "# target_X = df.loc[\"2020-01-30 00:00:00\":]\n",
    "# target_X = target_X.drop(\"result\", axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "# df = df.drop(df.index[-target_X.shape[0]:])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[[0.         0.70663074 0.15497835 ... 0.         0.         0.        ]\n",
      " [0.         0.70498428 0.16969697 ... 0.         0.         0.        ]\n",
      " [0.         0.70498428 0.08398268 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.23259991 0.08138528 ... 0.         0.         0.        ]\n",
      " [0.         0.22541536 0.07878788 ... 0.         0.         0.        ]\n",
      " [0.         0.22212244 0.12207792 ... 0.         0.         0.        ]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "TRAIN_SPLIT = int(len(df.index) * 1)\n",
    "scaler = MinMaxScaler().fit(df)\n",
    "values = scaler.transform(df)\n",
    "save = values\n",
    "print(values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "train = values[:20449, :]\n",
    "test = values[20449:20546, :]\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, 1:], train[:, 0]\n",
    "test_X, test_y = test[:, 1:], test[:, 0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(20449, 1, 9) (20449,) (97, 1, 9) (97,)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "def plot_train_history(history, title):\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(len(loss))\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "def show_plot(plot_data, delta, title):\n",
    "    labels = ['History', 'True Future', 'Model Prediction']\n",
    "    marker = ['.-', 'rx', 'go']\n",
    "    time_steps = create_time_steps(plot_data[0].shape[0])\n",
    "    if delta:\n",
    "        future = delta\n",
    "    else:\n",
    "        future = 0\n",
    "\n",
    "    plt.title(title)\n",
    "    for i, x in enumerate(plot_data):\n",
    "        if i:\n",
    "            plt.plot(future, plot_data[i], marker[i], markersize=10,\n",
    "                     label=labels[i])\n",
    "        else:\n",
    "            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
    "    plt.legend()\n",
    "    plt.xlim([time_steps[0], (future + 5) * 2])\n",
    "    plt.xlabel('Time-Step')\n",
    "    return plt\n",
    "\n",
    "\n",
    "def create_time_steps(length):\n",
    "    return list(range(-length, 0))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "def multi_step_plot(history, true_future, prediction):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    num_in = create_time_steps(len(history))\n",
    "    num_out = len(true_future)\n",
    "\n",
    "    plt.plot(num_in, np.array(history[:, 1]), label='History')\n",
    "    plt.plot(np.arange(num_out) / STEP, np.array(true_future), 'bo',\n",
    "             label='True Future')\n",
    "    if prediction.any():\n",
    "        plt.plot(np.arange(num_out) / STEP, np.array(prediction), 'ro',\n",
    "                 label='Predicted Future')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "    plt.savefig(\"output.png\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "def fit_by_batch(X, y, batch_size):\n",
    "    n_batches_for_epoch = X.shape[0]//batch_size\n",
    "    for i in range(n_batches_for_epoch):\n",
    "        index_batch = range(X.shape[0])[batch_size*i:batch_size*(i+1)]\n",
    "        X_batch =X[index_batch][0].toarray()[0] #from sparse to array\n",
    "        X_batch=X_batch.reshape(1,X_batch.shape[0],1 ) # to 3d array\n",
    "        y_batch = y[index_batch,][0]\n",
    "        yield(np.array(X_batch),y_batch)\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "\n",
    "multi_step_model = tf.keras.models.Sequential()\n",
    "multi_step_model.add(tf.keras.layers.GRU(300, \n",
    "                                          return_sequences=True,\n",
    "                                          input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "multi_step_model.add(tf.keras.layers.ReLU())\n",
    "multi_step_model.add(tf.keras.layers.Dense(300))\n",
    "multi_step_model.add(tf.keras.layers.LeakyReLU())\n",
    "multi_step_model.add(tf.keras.layers.Dense(1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_3 (GRU)                  (None, 1, 300)            279900    \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 1, 300)            0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1, 300)            90300     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 1, 300)            0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1, 1)              301       \n",
      "=================================================================\n",
      "Total params: 370,501\n",
      "Trainable params: 370,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "multi_step_model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[+] Available GPUs\n",
      "[]\n",
      "[+] Available multiple GPU not found... Just use CPU! XD\n",
      "Epoch 1/1000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00005, saving model to ../23_checkpoint.keras\n",
      "80/80 - 1s - loss: 2.1888e-04 - val_loss: 4.8800e-05 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00005 to 0.00004, saving model to ../23_checkpoint.keras\n",
      "\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "80/80 - 1s - loss: 5.5336e-05 - val_loss: 3.7502e-05 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00004 to 0.00004, saving model to ../23_checkpoint.keras\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "80/80 - 1s - loss: 5.3423e-05 - val_loss: 3.6946e-05 - lr: 8.0000e-04\n",
      "Epoch 4/1000\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00004 to 0.00004, saving model to ../23_checkpoint.keras\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "80/80 - 1s - loss: 5.3533e-05 - val_loss: 3.6334e-05 - lr: 6.4000e-04\n",
      "Epoch 5/1000\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00004\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "80/80 - 1s - loss: 5.4887e-05 - val_loss: 3.8755e-05 - lr: 5.1200e-04\n",
      "Epoch 6/1000\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00004\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
      "80/80 - 1s - loss: 5.2752e-05 - val_loss: 3.6375e-05 - lr: 4.0960e-04\n",
      "Epoch 7/1000\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00004\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
      "80/80 - 1s - loss: 5.3198e-05 - val_loss: 3.7791e-05 - lr: 3.2768e-04\n",
      "Epoch 8/1000\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00004\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020971521735191345.\n",
      "80/80 - 1s - loss: 5.2279e-05 - val_loss: 3.7991e-05 - lr: 2.6214e-04\n",
      "Epoch 9/1000\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00004\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00016777217388153076.\n",
      "80/80 - 1s - loss: 5.1602e-05 - val_loss: 3.8803e-05 - lr: 2.0972e-04\n",
      "Epoch 10/1000\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00004 to 0.00004, saving model to ../23_checkpoint.keras\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00013421773910522462.\n",
      "80/80 - 1s - loss: 5.1926e-05 - val_loss: 3.6289e-05 - lr: 1.6777e-04\n",
      "Epoch 11/1000\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00004\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010737419361248613.\n",
      "80/80 - 1s - loss: 5.1116e-05 - val_loss: 3.7248e-05 - lr: 1.3422e-04\n",
      "Epoch 12/1000\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00004\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.589935605414213e-05.\n",
      "80/80 - 1s - loss: 5.1334e-05 - val_loss: 3.7335e-05 - lr: 1.0737e-04\n",
      "Epoch 13/1000\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00004 to 0.00004, saving model to ../23_checkpoint.keras\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.871948717162013e-05.\n",
      "80/80 - 1s - loss: 5.0946e-05 - val_loss: 3.5882e-05 - lr: 8.5899e-05\n",
      "Epoch 14/1000\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00004\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 5.497558740898967e-05.\n",
      "80/80 - 1s - loss: 5.0911e-05 - val_loss: 3.7848e-05 - lr: 6.8719e-05\n",
      "Epoch 15/1000\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00004\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 4.398046876303852e-05.\n",
      "80/80 - 1s - loss: 5.1272e-05 - val_loss: 3.7222e-05 - lr: 5.4976e-05\n",
      "Epoch 16/1000\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00004\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.518437442835421e-05.\n",
      "80/80 - 1s - loss: 5.0937e-05 - val_loss: 3.6967e-05 - lr: 4.3980e-05\n",
      "Epoch 17/1000\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00004\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 2.8147498960606756e-05.\n",
      "80/80 - 1s - loss: 5.0800e-05 - val_loss: 3.7194e-05 - lr: 3.5184e-05\n",
      "Epoch 18/1000\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00004\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 2.25179988774471e-05.\n",
      "80/80 - 1s - loss: 5.0655e-05 - val_loss: 3.6454e-05 - lr: 2.8147e-05\n",
      "Epoch 19/1000\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00004\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.8014399392995985e-05.\n",
      "80/80 - 1s - loss: 5.0623e-05 - val_loss: 3.6748e-05 - lr: 2.2518e-05\n",
      "Epoch 20/1000\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00004\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.4411519805435093e-05.\n",
      "80/80 - 1s - loss: 5.0708e-05 - val_loss: 3.6377e-05 - lr: 1.8014e-05\n",
      "Epoch 21/1000\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00004\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.1529216135386379e-05.\n",
      "80/80 - 1s - loss: 5.0741e-05 - val_loss: 3.6808e-05 - lr: 1.4412e-05\n",
      "Epoch 22/1000\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00004\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "80/80 - 1s - loss: 5.0745e-05 - val_loss: 3.7331e-05 - lr: 1.1529e-05\n",
      "Epoch 23/1000\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00004\n",
      "80/80 - 1s - loss: 5.0603e-05 - val_loss: 3.7279e-05 - lr: 1.0000e-05\n",
      "Epoch 24/1000\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00004\n",
      "80/80 - 1s - loss: 5.0548e-05 - val_loss: 3.7231e-05 - lr: 1.0000e-05\n",
      "Epoch 25/1000\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00004\n",
      "80/80 - 1s - loss: 5.0554e-05 - val_loss: 3.6833e-05 - lr: 1.0000e-05\n",
      "Epoch 26/1000\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00004\n",
      "80/80 - 1s - loss: 5.0560e-05 - val_loss: 3.6800e-05 - lr: 1.0000e-05\n",
      "Epoch 27/1000\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00004\n",
      "80/80 - 1s - loss: 5.0511e-05 - val_loss: 3.7046e-05 - lr: 1.0000e-05\n",
      "Epoch 28/1000\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00004\n",
      "80/80 - 1s - loss: 5.0528e-05 - val_loss: 3.7308e-05 - lr: 1.0000e-05\n",
      "Epoch 29/1000\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00004\n",
      "80/80 - 1s - loss: 5.0597e-05 - val_loss: 3.6578e-05 - lr: 1.0000e-05\n",
      "Epoch 30/1000\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00004\n",
      "80/80 - 1s - loss: 5.0558e-05 - val_loss: 3.6998e-05 - lr: 1.0000e-05\n",
      "Epoch 31/1000\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00004\n",
      "80/80 - 1s - loss: 5.0579e-05 - val_loss: 3.6844e-05 - lr: 1.0000e-05\n",
      "Epoch 32/1000\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00004\n",
      "80/80 - 1s - loss: 5.0586e-05 - val_loss: 3.6377e-05 - lr: 1.0000e-05\n",
      "Epoch 33/1000\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00004\n",
      "80/80 - 1s - loss: 5.0587e-05 - val_loss: 3.6579e-05 - lr: 1.0000e-05\n",
      "Epoch 34/1000\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00004\n",
      "80/80 - 1s - loss: 5.0758e-05 - val_loss: 3.7024e-05 - lr: 1.0000e-05\n",
      "Epoch 35/1000\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00004\n",
      "80/80 - 1s - loss: 5.0555e-05 - val_loss: 3.7249e-05 - lr: 1.0000e-05\n",
      "Epoch 36/1000\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00004\n",
      "80/80 - 1s - loss: 5.0506e-05 - val_loss: 3.8142e-05 - lr: 1.0000e-05\n",
      "Epoch 37/1000\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00004\n",
      "80/80 - 1s - loss: 5.0601e-05 - val_loss: 3.7482e-05 - lr: 1.0000e-05\n",
      "Epoch 38/1000\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00004\n",
      "80/80 - 1s - loss: 5.0876e-05 - val_loss: 3.8302e-05 - lr: 1.0000e-05\n",
      "Epoch 39/1000\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00004\n",
      "80/80 - 1s - loss: 5.0536e-05 - val_loss: 3.6886e-05 - lr: 1.0000e-05\n",
      "Epoch 40/1000\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00004\n",
      "80/80 - 1s - loss: 5.0576e-05 - val_loss: 3.6567e-05 - lr: 1.0000e-05\n",
      "Epoch 41/1000\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00004\n",
      "80/80 - 1s - loss: 5.0560e-05 - val_loss: 3.6477e-05 - lr: 1.0000e-05\n",
      "Epoch 42/1000\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00004\n",
      "80/80 - 1s - loss: 5.0580e-05 - val_loss: 3.6535e-05 - lr: 1.0000e-05\n",
      "Epoch 43/1000\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00004\n",
      "80/80 - 1s - loss: 5.0560e-05 - val_loss: 3.7558e-05 - lr: 1.0000e-05\n",
      "Epoch 44/1000\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00004\n",
      "80/80 - 1s - loss: 5.0512e-05 - val_loss: 3.7111e-05 - lr: 1.0000e-05\n",
      "Epoch 45/1000\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00004\n",
      "80/80 - 1s - loss: 5.0535e-05 - val_loss: 3.7428e-05 - lr: 1.0000e-05\n",
      "Epoch 46/1000\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00004\n",
      "80/80 - 1s - loss: 5.0589e-05 - val_loss: 3.7263e-05 - lr: 1.0000e-05\n",
      "Epoch 47/1000\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00004\n",
      "80/80 - 1s - loss: 5.0651e-05 - val_loss: 3.7433e-05 - lr: 1.0000e-05\n",
      "Epoch 48/1000\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00004\n",
      "80/80 - 1s - loss: 5.0530e-05 - val_loss: 3.7047e-05 - lr: 1.0000e-05\n",
      "Epoch 49/1000\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00004\n",
      "80/80 - 1s - loss: 5.0519e-05 - val_loss: 3.6789e-05 - lr: 1.0000e-05\n",
      "Epoch 50/1000\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00004\n",
      "80/80 - 1s - loss: 5.0435e-05 - val_loss: 3.8921e-05 - lr: 1.0000e-05\n",
      "Epoch 51/1000\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00004\n",
      "80/80 - 1s - loss: 5.0603e-05 - val_loss: 3.7096e-05 - lr: 1.0000e-05\n",
      "Epoch 52/1000\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00004\n",
      "80/80 - 1s - loss: 5.0468e-05 - val_loss: 3.6895e-05 - lr: 1.0000e-05\n",
      "Epoch 53/1000\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00004\n",
      "80/80 - 1s - loss: 5.0488e-05 - val_loss: 3.7567e-05 - lr: 1.0000e-05\n",
      "Epoch 54/1000\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00004\n",
      "80/80 - 1s - loss: 5.0474e-05 - val_loss: 3.7330e-05 - lr: 1.0000e-05\n",
      "Epoch 55/1000\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/unajun/Library/Python/3.7/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-96-5e9dcca76df0>\", line 42, in <module>\n",
      "    verbose=2, shuffle=True, callbacks=callbacks)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 848, in fit\n",
      "    tmp_logs = train_function(iterator)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 611, in _call\n",
      "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2420, in __call__\n",
      "    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1665, in _filtered_call\n",
      "    self.captured_inputs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1746, in _call_flat\n",
      "    ctx, args, cancellation_manager=cancellation_manager))\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 598, in call\n",
      "    ctx=ctx)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\n",
      "    inputs, attrs, num_outputs)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/unajun/Library/Python/3.7/lib/python/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/unajun/Library/Python/3.7/lib/python/site-packages/IPython/core/ultratb.py\", line 1148, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/unajun/Library/Python/3.7/lib/python/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/unajun/Library/Python/3.7/lib/python/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 732, in getmodule\n",
      "    for modname, module in list(sys.modules.items()):\n",
      "KeyboardInterrupt\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "EVALUATION_INTERVAL = 200\n",
    "\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "\n",
    "def get_gpu_num():\n",
    "    return len(get_available_gpus())\n",
    "\n",
    "\n",
    "path_checkpoint = '../23_checkpoint.keras'\n",
    "callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
    "                                      monitor='val_loss',\n",
    "                                      verbose=1,\n",
    "                                      save_weights_only=True,\n",
    "                                      save_best_only=True)\n",
    "\n",
    "callback_early_stopping = EarlyStopping(monitor='val_loss', patience=80, verbose=1)\n",
    "\n",
    "callback_tensorboard = TensorBoard(log_dir='../23_logs/', histogram_freq=0, write_graph=False)\n",
    "callback_reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.8, min_lr=1e-5, patience=0,verbose=1)\n",
    "callbacks = [callback_early_stopping, callback_checkpoint, callback_tensorboard, callback_reduce_lr]\n",
    "\n",
    "print(f\"[+] Available GPUs\")\n",
    "print(get_available_gpus())\n",
    "\n",
    "if get_gpu_num() < 2:\n",
    "    print(f\"[+] Available multiple GPU not found... Just use CPU! XD\")\n",
    "else:\n",
    "    print(f\"[+] {get_gpu_num()} GPUs found! Setting to GPU model...\")\n",
    "    multi_step_model = multi_gpu_model(multi_step_model, gpus=get_gpu_num())\n",
    "\n",
    "multi_step_model.compile(optimizer=tf.keras.optimizers.Adam(), loss='mse')\n",
    "\n",
    "history = multi_step_model.fit(train_X, train_y, epochs=EPOCH, batch_size=32 * 8,validation_data=(test_X, test_y),\n",
    "                               verbose=2, shuffle=True, callbacks=callbacks)\n",
    "\n",
    "try:\n",
    "    multi_step_model.load_weights(path_checkpoint)\n",
    "except Exception as error:\n",
    "    print(\"Error trying to load checkpoint.\")\n",
    "    print(error)\n",
    "\n",
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "pyplot.savefig(\"test.png\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from numpy import concatenate\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# make a prediction\n",
    "yhat = multi_step_model.predict(test_X)[:, :, 0]\n",
    "print(yhat)\n",
    "# make a prediction\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = concatenate((yhat, test_X), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:, 0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = concatenate((test_y, test_X), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:, 0]\n",
    "# calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.6f' % rmse)\n",
    "print('Test MAE: %.6f' % mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test nMAE: %.6f' % (mean_squared_error(inv_y, inv_yhat) / 7028))\n",
    "\n",
    "pyplot.plot([x for x in range(1000)], inv_y[-1000:], 'b', label='true')\n",
    "pyplot.plot([x for x in range(1000)], inv_yhat[-1000:], 'r', label='pred')\n",
    "pyplot.legend(loc='upper left')\n",
    "pyplot.savefig(\"out.png\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}